---
title: "How Computers Work"
tags:
  - mathjax
use_math: true
---

# INTRODUCTION

For generations, large systems have been built in layers, with well-defined interfaces between each layer. This design allows any old layer to be replaced with a newer, better implementation, without disrupting the rest of the system. Eventually, though, the once-good design becomes too outdated and the whole lot must be replaced with a new design and a new implementation that better fits current needs.

But in recent years, computing technologies have advanced at a ferocious pace and IT systems have become immensely complex. To keep pace with the rapidly changing worldwide market, the IT industry takes the expedient approach: it builds newer, more complex technologies atop outmoded, simple technologies, instead of taking the correct—but costly, time-consuming, and disruptive—approach of replacing old technologies with newer, better ones. This situation is due in part to the economics and practices of modern software development and in part to the deeply enmeshed nature of some older, shortsightedly planned, hastily developed technologies.

The progressively deep and complex layering moves programmers farther and farther away from the hardware, so much so that many experienced programmers today no longer understand something as fundamental as the system clock phases or the instruction execution cycle. Surely, a software developer can have a profitable career working on social media and line of business applications without ever having peered inside a computer case. But software development is more than captivating user interfaces and undocumented REST requests. To be effective, a programmer must, at the very least, possess an accurate mental model of how a computer executes programmes.

I wrote this article for programmers who work exclusively with software, away from hardware. My goal here is modest; I aim to explain the most basic aspects of computing: how a computer is built from disparate subsystems, how these subsystems collectively perform simple calculations, and how complex system behaviour emerges from combinations of just two symbols, a `0` and a `1`. But who really cares about such lowly matters? Every IT practitioner should, I say. Recognising that things happen when they do is a sign of intelligence, a commonplace phenomenon within [Animalia](https://en.wikipedia.org/wiki/Animal). But the drive to understand why things are as they are— that is a mark of intellect, a quality unique to [Homo sapiens](https://en.wikipedia.org/wiki/Human).

# COMPUTING

We humans possess a unique ability to plan for the future. But to exercise this ability, we are obliged to assume the burden continually to observe, calculate, and predict. To ease that burden, we have amassed a collection of ingenious tools and techniques. Thousands of years ago, we used [stones to build computers](https://en.wikipedia.org/wiki/Stonehenge) that can measure the passage of seasons, so that we may plan harvest thereby saving the tribe from starvation, come winter. Today, we use silicon, stones in a different guise, to make supercomputers that can predict the paths of winter storms, so that we may save the lives of millions of coastal residents.

The term "computer" can be applied broadly to any mechanical means that can be used in calculating. So, [beans](https://en.wikipedia.org/wiki/Bean_machine) and [bones](https://en.wikipedia.org/wiki/Ishango_bone) are as much "computing devices" as are [slide rules](https://en.wikipedia.org/wiki/Slide_rule) and [supercomputers](https://en.wikipedia.org/wiki/Supercomputer). And until the 1940s, a "computer" was the [person who performed numerical computations](https://en.wikipedia.org/wiki/Computer_(job_description)) using the slide rule. In this article, "computer" means the modern digital computer. With all its sophistication, the modern digital computer is still a mechanical aid for calculating. Yet, simple calculations like additions and subtractions, when mixed with human ingenuity, achieves marvellous, maybe even miraculous, feats like air traffic control and artificial intelligence (AI).

## *from bits to thoughts*

***bits, bytes, and booleans***—The name "digital computer" derives from the fact that this device can manipulate only *b*inary dig*its* ([bits](https://en.wikipedia.org/wiki/Bit)). A bit can be either `0` or `1`. The simplest modern computer is the 1980s home computer. Such a computer is called the "8-bit" machine, because it manipulates an 8-bit quantity (a *byte*) at a time. Boolean (true or false) values are represented using bits within a byte. But a particular bit can be access only through its containing byte. This is the consequence of byte-at-a-time processing scheme. The byte is also known as the *word* on an 8-bit machine. But on a 16-bit, 32-bit, or 64-bit computer, a word is respectively those quantities. That is, the word matches the CPU's native data size.

The 8-bit byte, despite being the smallest data container, holds a mythical status in computing. At one time, 9 bits, 7 bits, and even 4 bits were used as the basic unit. But today, every modern CPU uses the 8-bit byte as the basic unit of computation. These days, even the simplest of computers, like Raspberry Pi, iPhone, etc., employ 64-bit CPUs. But the 8-bit CPU is not extinct; the popular [Arduino](https://en.wikipedia.org/wiki/Arduino) single-board microcontroller platform uses an 8-bit CPU. So, we focus exclusively on the 8-bit CPU in this article. That keeps the presentation simple, yet relevant.

***numbers and letters***—A decimal digit can represent 0, 1, 2, ..., 9, but a binary digit can represent only 0 up to 1. The decimal number system requires multiple digits to represent a number greater than 9. Likewise, the binary number system requires multiple digits to represent a number greater than 1. Hence, decimal 0 is represented as binary `0`, 1 as `1`, 2 as `10`, 3 as `11`, 4 as `100`, and so on. The minimum value a byte can represent is `00000000` (decimal 0), and the maximum value is `11111111` (decimal 255). Thus, a byte can represent $2^8 = 256$ different values. Numbers greater than 255 must be represented by placing two bytes, side by side. An 8-bit CPU manipulates 16-bit quantities one byte at a time. Since the 1940s, the dawn of the electronic computer, numbers have been represented and manipulated using various formats. But all modern computers use [2's complement](https://en.wikipedia.org/wiki/Two%27s_complement) to represent signed integers and [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754) floating-point format to approximate real numbers.

Computers use ASCII and Unicode standard formats to represent characters. [ASCII](https://en.wikipedia.org/wiki/ASCII) was adopted as the standard binary format in the 1960s, with the advent of the modern digital computer. The numbers, letters, and myriad symbols that appear on the American standard keyboard are mapped to the ASCII symbol table. ASCII uses one byte to represent one character, so it can represent up to 256 different symbols. For instance, the word `"hi"` in ASCII binary representation requires two bytes: `01101000` for `h` and `01101001` for `i`. But tens of thousands of symbols are needed to represent all the languages in the world. The 1980s personal computer revolution prompted various efforts to represent complex characters in multiple bytes, which culminated in the [Unicode](https://en.wikipedia.org/wiki/Unicode) standard of the 1990s. The old ASCII standard 8-bit character format has been subsumed into Unicode as [UTF-8](https://en.wikipedia.org/wiki/UTF-8).

Note that the CPU recognises only the following types: byte value, unsigned integer value, signed integer value, floating-point value, and memory address. As stated above, a character value is represented as an unsigned integer, and a string value is represented as a sequence of character values. All other data types are treated as byte sequences.

***addition, subtraction, multiplication, and division***—With standard formats like 2's complement, IEEE 754, and Unicode, the computer can represent numbers and letters. But representation is one thing, computation is quite another matter. The most basic calculation that a digital computer performs is the addition. The CPU adds numbers using a circuit with an unimaginative name—the *adder*. Two binary integers are added just like two decimal integers. In the decimal number system, 9 + 1 = 10, because adding 1 to the maximal value that a decimal digit can represent, namely 9, overflows the digit, and hence must be represented by two digits. Similarly, in the binary number system, 1 + 1 = 10 (decimal 2), because adding 1 to the maximal value that a binary digit can represent, namely 1, overflows the digit, and hence must be represented by two digits. When bits are assembled into an 8-bit quantity, this quantity can represent a total of 256 different values.

But what about negative numbers? In the 2's complement format, the left-most bit of a binary quantity represents the negative sign: `0` is positive, and `1` is negative. Hence, `00000001` is +1, and `10000001` is -1. Because this format reserves the left-most bit for sign representation, the largest positive integer an 8-bit quantity can represent is `01111111` (decimal 127) and the smallest negative number is `11111111` (decimal -128). An 8-bit singed integer, therefore, can represent 128 negative values, the `00000000` which is conventionally treated as a positive value, and 127 other positive values, giving a total of 256 different values. In other words, an 8-bit quantity, singed or unsigned, can represent 256 different integer values. So, an unsigned byte can represent decimals in the closed interval $[0, 255]$, and a signed byte can represent decimals in the closed interval $[-128, +127]$. Using the 2's complement representation of negative integers, the integer adder unit can compute subtraction: 9 + (-7) = 2.

Integer arithmetic (`add`, `sub`, `mul`, `div`) is performed by the [arithmetic logic unit](https://en.wikipedia.org/wiki/Arithmetic_logic_unit#INTOP) (ALU) inside the CPU. The ALU also performs boolean logic (`not`, `and`, `or`)—hence, the "logic" in its name. The 8-bit CPUs performed floating-point arithmetic in software, because adding specialised, floating-point hardware would have made the machines uneconomical for the home market. More powerful 16-bit CPUs of that era, Intel 8086 for example, also lacked floating-point hardware, but it can be augmented with an optional add-on [floating-point unit](https://en.wikipedia.org/wiki/Floating-point_unit) (FPU) that performs floating-point arithmetic (`fadd`, `fsub`, `fmul`, `fdiv`). The FPU was then known as the [floating-point coprocessor](https://en.wikipedia.org/wiki/Intel_8087). Modern CPUs, starting with the [Intel 80486](https://en.wikipedia.org/wiki/Intel_80486) processor, have integrated FPU hardware on chip.

***decision and repetition***—A sequence of bits (`0`s and `1`s) can represent letters and numbers. And the adder unit can add and subtract numbers. Programmes must make decisions based on conditions:

```assembly
  ...
  cond a < b ⇒ jump con ⇏ jump alt
  ...
con: ; consequent code block
  ...
alt: ; alternative code block
  ...
```

For clarity and concision, the above code is written in pseudo assembly language. The conditional branch is computed by comparing two integers `a` and `b`. The CPU compares these two integers by first computing the subtraction `a - b`, then checking if the result is negative, zero, or positive. A negative means `a < b`; a zero means `a = b`; positive means `a > b`. By taking one execution path or another on the basis of this result, the CPU accomplishes conditional branching.

Repetitive execution of a block of code, also known as looping, is accomplished by jumping back to the top of the block, based on a branching condition located at the bottom of the block:

```assembly
  ...
loop: ; start of loop
  i ← 0
  ...
  cond i < n ⇒ jump loop ⇏ jump end
end: ; end of loop
  ...
```

Above, integer `i` is the loop index which is initialised to 0 at the top of the loop, and `n` is the maximum number of iterations. When `i = n`, the loop termination condition is met, so the CPU jumps out of the loop, instead of jumping back to the top of the loop.

***algorithm and intelligence***—A CPU equipped with boolean logic operations, character and string operations, integer and floating-point operations, and branching and looping constructs is capable of simple, but general, computations. For example, exponentiation $b^e = b \cdot b \cdot ... \cdot b$ is computed as $e$ repeated multiplications of $b$, and factorial $n! = (n - 0) \cdot (n - 1) \cdot ... \cdot 2 \cdot 1$ is computed as $n$ repeated multiplications of $n - i$ where $i$ ranges over the closed interval  $[0, n-1]$. The exponentiation algorithm uses looping and multiplication, and the factorial algorithm uses looping, subtraction, and multiplication. A more complex computation, $sin(a)$ where $a$ is an angle in radians, can be computed using Taylor series with sufficiently large number of iterations $n$:

<div>
$$
sin(a) = \sum_{i=0}^{n} \frac{(-1)^i}{(2i+1)!} a^{2i+1}
$$
</div>
The sine algorithm and its constituents, the exponentiation and the factorial, use looping, addition, subtraction, multiplication, and division. In this way, arbitrarily complex computations can be constructed from combinations of simple ones.

Algorithms like these are implemented as functions that take inputs and return results. Functions may be invoked using the `call` instruction from anywhere in the code. A function call, just like a branch, alters programme execution flow. But there is a difference. Where as upon encountering a branch the CPU jumps immediately to the designated code block and the execution continues from there, when calling a function the CPU must assemble the input values for the function to use and also must arrange for the function to return its results to the calling code, before jumping into the function's code block. And when the function finishes, execution returns to the caller.

More sophisticated functions, like [sorting](https://en.wikipedia.org/wiki/Sorting_algorithm), [searching](https://en.wikipedia.org/wiki/Search_algorithm), and [graph](https://en.wikipedia.org/wiki/Graph_theory) algorithms, are implemented by composing simpler functions. Complicated AI algorithms like [chess](https://en.wikipedia.org/wiki/Computer_chess) and [Go](https://en.wikipedia.org/wiki/Computer_Go) use sorting, searching, and graph algorithms. Thus arose semblance of human thought and i from streams of two symbols, a `0` and a `1`.

By its very name, "artificial intelligence" is not intelligence, but an imitation thereof. Thinking, feeling machines have not been invented, because humans do not yet understand the nature of human intellect. There are two broad categories of AI algorithms: rule-based AI and connectionist AI. *Rule-based* AI is used when the problem is well understood and the solution can be expressed as a small set of clearly defined rules. Board game algorithms are prime examples of rule-based AI. *Connectionist* AI, more popularly known as neural networks (NNs), is used when the problem is poorly understood, has a large set of potential solutions which makes it impracticable to be described in terms of rules, and a vast amount of input data can be collected beforehand. Autonomous vehicle navigation, crowd behaviour modelling, and similar open-ended problems are solved using NNs.

# SOFTWARE

In this section, we explore the software aspects of computing. Software that is the closest to hardware is the machine code—`0`s and `1`s. Machine code is generated from assembly language, which comprises human-readable mnemonics, like `add`, `sub`, `jump`, `call`, and so on. There is one-to-one correspondence between machine instructions and assembly mnemonics. That is, an assembly programme can be assembled into its equivalent machine code programme, and the machine code programme can be disassembled back into the assembly programme.

Every CPU family implements its own set of instructions. This is also known as the [instruction set architecture](https://en.wikipedia.org/wiki/Instruction_set_architecture) (ISA) of the CPU. A typical ISA comprises instructions for clock configuration, for arithmetic and logic, for memory access, and for input and output. The ISA is the programmer-visible aspects of the microarchitecture.

In a typical, 1980s 8-bit home computer system, memory was the scarcest resource. Naturally, memory access methods on these CPUs were the most complex of all programming models. A typical 8-bit CPU supports several [memory addressing modes](http://www.6502.org/users/andre/65k/af65002/af65002admodes.html): immediate, absolute, relative, implied, indirect, indexed, and combinations thereof. The intricacies of these addressing modes are outmoded today, and they are not relevant to our discussions. But recognise that this complexity enabled a low-end CPU to do as much work as possible using as few bits of instructions as necessary. This increased code density of the ISA and thus reduced memory requirements of programmes.

***procedure calls***—The only abstraction mechanisms that assembly language offers are the ability for the programmer to declare variables and to organise code into procedures. And the most important procedures on an 8-bit home computer were the interrupt handlers and the system service procedures, which were all stored in the ROM, alongside the ubiquitous BASIC interpreter.

When the CPU powers up, it performs a system-wide hardware reset. Hardware reset clears CPU's internal registers (accumulator, index registers, programme counter, stack pointer, processor status register, etc.) and also clears peripherals' registers (configuration registers, data transfer registers, etc.). This puts the whole system in a known, initial state. It then executes the reset service procedure. The CPU knows where to find this procedure, because the entry address of the procedure is conventionally stored in the ROM at a well-known address, such as `0x0000` or `0xFFFF`. The reset procedure by checks the health of memory and I/O devices and completes the system initialisation by setting up the interrupt service vector table, putting processor status register bits to appropriate initial values, and so on. Once the system has been fully initialised, the reset procedure starts the BASIC interpreter stored in the ROM. The BASIC interpreter served not only as the language interpreter, it also served as a programme editor and a simple command line interpreter to access system services, such as loading programmes from external storage devices.

The programmer defines a procedure in assembly by grouping a sequence of instructions under a label. The following is a procedure (a function, actually), written in pseudo assembly, that doubles its integer argument:

```assembly
  ...
dbl: ; procedure that doubles input integer
  mul 2 ; multiply argument value in ACC with 2; store result in ACC
  ret   ; load PC with return addr from stack
  ...
```

This procedure can be invoked as follows:

```assembly
  ...
  a ← 13   ; load ACC with argument value 13
  call dbl ; push return addr onto stack; load PC with dbl's addr
  a → m    ; store result value in ACC into memory for use later
  ...
```

The above pseudo assembly code is stripped to its essentials so as to highlight how procedures are defined and invoked in assembly, using instruction addresses, registers, `call` instruction, `ret` instruction, and stack memory. To call the `dbl` procedure, the caller passes 13 as the argument to `dbl` via the accumulator register, and `dbl` returns its result to the caller via the accumulator, as well. The `call` instruction automatically pushes onto the top of the stack the return address, which is the address of the instruction immediately after `call`, then jumps to the first instruction in `dbl`. Upon completion, the `ret` instruction at the end of  `dbl` automatically loads the programme counter with the return address at the top of the stack, then jumps to the instruction after `call`. Programme execution continues from there. The above code sequence would be written in C as follows:

```C
...
int dbl(int x) { return 2 * x; }
...
m = dbl(13);
...
```

Every time a procedure is called, some data are pushed onto the stack, thereby preserving the pre-call state of the CPU. If the current procedure calls another, more state will be pushed onto the stack. Hence, a runaway recursive call will exhaust the stack memory, and will throw the [stack overflow exception](https://en.wikipedia.org/wiki/Stack_buffer_overflow).

***input/output***—Although the BASIC language is far removed from the underlying processor, special language constructs provide direct read/write access to registers inside the CPU and the peripherals. This gave hobbist programmers efficient and precise ways to initialise, configure, and control peripherals, like video, audio, and storage. Some BASIC implementations even accept assembly instructions embedded inside the BASIC programme.

Perhaps the simplest programming model for interacting with peripherals is the [memory-mapped I/O](https://en.wikipedia.org/wiki/Memory-mapped_I/O). A processor that implements the memory-mapped I/O scheme allocates portions of the address space to ROM, RAM, and I/O registers. This method allows the programmer to access both memory and peripherals uniformly, using memory access instructions and their sophisticated addressing modes. This approach is simple, effective, and convenient. But because significant portions of the address space are allotted to ROM and I/O, the total addressable RAM is much reduced. All 8-bit machines were equipped with 16-bit address bus capable of addressing 65 KB, and the early machines were equipped with only 16 KB or 32 KB of RAM. In this minimal configuration, the conveniences of memory-mapped I/O outweighed the loss of usable RAM addresses. The popularity of memory-mapped I/O is ever greater, now: the [ARM](https://en.wikipedia.org/wiki/ARM_architecture), arguably the most successful processor architecture in history, uses this scheme in its [Cortex-M](https://developer.arm.com/ip-products/processors/cortex-m) line of modern microcontrollers. And because the Cortex-M's 32-bit address bus has 4 GB of address space and most microcontrollers are equipped with a few hundred KB of ROM and a few tens of KB of SRAM, loss of a few addresses to memory-mapped I/O devices is not a problem.

***interrupt services***—A classic 8-bit CPU handles I/O in two ways: synchronous and asynchronous. In synchronous I/O mode, the CPU would send a block of data to the peripheral, say a storage device, and wait for this much-slower device to complete receiving the data block, before sending another block to it. The time the CPU spent waiting for the peripheral could not be put to profitable use, such as handling another device or performing calculations.

In asynchronous I/O mode, the CPU would fill a buffer, either in RAM or on the peripheral, with a block of data, issue a write instruction, and move on to perform other tasks. When the peripheral has completed receiving the block of data, it sends an interrupt signal to the CPU. When an interrupt is received by the CPU, it completes the currently executing instruction, but immediately jumps to the service handler procedure associated with the interrupt. The service handler then performs another chunk of the I/O task, and upon completion, returns to the interrupted code and resumes execution. In other words, interrupt service handler procedure is invoked just like a normal procedure, as mentioned above. Indeed, the reset service procedure mentioned above is the service handler procedure associated with the reset interrupt.

There are two types of interrupts: maskable interrupt request (IRQ) and non-maskable interrupt request (NMI). Some interrupt service handler procedures are critical to the proper functioning of the CPU, and hence they must be attended to, immediately. But most interrupts, including I/O varieties, are maskable IRQs. IRQs are assigned priorities, based on their importance to the proper functioning of the CPU. A higher priority IRQ (say, a keyboard input request) may interrupt the execution of a lower priority IRQ (say, a disc output request) service handler procedure. In this situation, the CPU will push the state of the disc write service handler procedure onto the stack, and jump to the service handler procedure associated with the keyboard input interrupt. The reset signal is the highest urgency NMI. Hence, no matter what the CPU is doing, when the reset signal arrives, it must immediately perform hardware reset, and invoke the reset interrupt service handler procedure.

***high-level languages***—The only data types an 8-bit processor can natively support are bit, byte, and 16-bit address. For 16-bit and 32-bit integer values, the CPU performs the operations two or four times, and it emulates floating-point values in software. On the other hand, a 32-bit processor, like the Cortex-M, supports natively bit, byte, 16-bit half-word, 32-bit word, and 32-bit address. And if the microcontroller is equipped with a floating-point hardware, as in the case of the [Cortex-M4F](https://www.st.com/en/microcontrollers-microprocessors/stm32f303cc.html) processor on the [RobotDyne Black Pill board](https://stm32-base.org/boards/STM32F303CCT6-RobotDyn-Black-Pill), it supports 32-bit single-precision floating-point value. The Cortex-M4 must perform 32-bit operations twice for 64-bit double-word integer and emulate in software 64-bit double-precision floating-point values its FPU supports only single-precision floating-point values.

The C programming language supports char (8-bit), short (16-bit), int (32-bit), long (64-bit), float (32-bit single-precision), and double (64-bit double-precision) data types, plus 32-bit or 64-bit pointer, depending on the processor architecture. Hence, C is the lowest (the closest to hardware), high-level (non-assembly) language in popular use. Indeed, many statements in C can either be translated directly into assembly or they can be expressed as combinations of a few mnemonics. Yet, C is far more convenient to use than assembly language, because C has a readable syntax and it supports high-order organisational constructs, like data structures, functions, and modules, without incurring undue overhead. Indeed, modern optimising C compilers can produce code that out performs hand-crafted assembly, in many situations. For this reason, most embedded programming is done in C, now.

As capabilities of processors grow, even minuscule microcontrollers can now tolerate the overhead associate with higher-level languages like C++, [Python](https://micropython.org/), [TypeScript](https://www.microsoft.com/en-us/research/uploads/prod/2019/09/static-typescript-draft2.pdf), [Rust](https://www.rust-lang.org/what/embedded), and [OCaml](https://link.springer.com/chapter/10.1007/978-3-319-19686-2_10). The growth of hardware capabilities will inevitably bring about the growth of software complexity. As software complexity increase, type safety, modularity, composability, and other benefits these higher-level languages offer will become necessities, in a not-so-distant future.

# HARDWARE

To keep the discussions short and simple, we shall talk about a fictional computer with minimal functionality: it has an 8-bit CPU; it has a memory system with ROM and RAM; it accepts user input via a hexadecimal keypad; it performs simple calculations like addition and subtraction; it displays results in binary format using 8 LEDs; but, it has no disc storage. We will not even discuss real 8-bit homes computers of the 1980s; simple as they were, they still are laden with real-life complexities that can muddy the presentation with too many details. And we will steer clear of complex, fully functional, modern computers, like the Raspberry Pi, mobile phone, tablet, laptop, and so on.

An implementation of the ISA in hardware is called the [microarchitecture](https://en.wikipedia.org/wiki/Microarchitecture). A simple CPU microarchitecture that can perform arithmetic and logic operations, call procedures, and handle interrupts is shown in the diagram below. It consists of the following: control unit (CU) which comprises control logic, timing logic, and instruction decoder (ID); instruction register (IR); interrupt controller (IC); status register (SR); arithmetic logic unit (ALU); accumulator (A); general purpose register (B); programme counter (PC); stack pointer (SP); internal data bus to which these components are attached; buffer circuit that isolates the internal data bus from the external data bus on the motherboard; and address lines that connect directly to the external address bus on the motherboard.

![microarchitecture](/Users/zwa/Documents/amenzwa.github.io/computing/diagrams/microarchitecture.png)

It is often said that the CPU is the brain of the computer. If that were so, the CU is the [cerebrum](https://en.wikipedia.org/wiki/Cerebrum); it controls all actions within the CPU, with the aid of ID, IC, and SR. Many circuitries inside the CPU as well as those in external components are built with [synchronous circuits](https://en.wikipedia.org/wiki/Synchronous_circuit). These components must be driven by the master clock signal in order for them to operate in unison. Timing signals go to every component in the system, so they are not shown in the above diagram. A typical classic 8-bit home computer ran at 1 MHz clock frequency. The clock signal is supplied by an external crystal.

The CPU chip is plugged into a socket on the motherboard, and is connected to the external address and data buses. These buses connect the CPU chip to ROM chips, RAM chips, video interface, audio interface, keyboard interface, and other peripherals located around the motherboards. The unidirectional address bus is 16 bits wide, so the CPU can address $2^{16} = 65,536$ memory locations.  Address signals travel from the CPU to the address decoders inside the peripherals. The bidirectional data bus is 8 bits wide. Data signals travel in both directions between the CPU and the peripherals. The external data bus is connected to the internal data bus via the buffer circuitry. The buffer isolates the two data buses and connects them only when the CPU needs to communicate with the external components. That is, the buffer prevents the incessant chatter amongst the CPU's internal components from leaking out onto the external data bus.

The primary function of the CPU is to run programmes. Programmes are sequences of machine instructions. The CPU runs the programme by fetching, decoding, and executing the first instruction, then the next, and the next, and so on. When an interrupt occurs, the CPU suspends the currently executing programme, and runs the interrupt service handler procedure. Once the interrupt has been serviced, the CPU resumes the suspended programme. We shall now examine these operational stages, individually.

***fetch***—For the CPU to execute an instruction, it must fetch the instruction from RAM (or ROM, in case of system services). The address of the instruction to be fetched must already be held in the PC. At the start of the fetch stage, the CU transmits the instruction address held in the PC along the address bus. The RAM's address decoder recognises this address as being one of its own. The RAM thus retrieves the contents of the addressed byte, and transmits the contents along the data bus. The CU knows that the transmitted byte is an instruction, so it stores the instruction in the IR.

***decode***—The CU now proceeds to the decode stage. The CU instructs the ID to interpret the contents of the IR. The circuitry inside the ID recognises the instruction type: arithmetic operation, logic operation, conditional branch, procedure call, memory access, I/O access, whatever.

***execute***—Having decoded the instruction, the CPU now enters the execution stage. If the instruction is a *memory read*, say one that uses absolute addressing, the CU knows that the instruction comprises three bytes—one byte for the opcode and two bytes for the operand address. The CU retrieves the addressed byte from RAM, and loads the contents into register A to be used as the operand. The CU then increments the PC by 3, so the PC now holds the address of the next instruction.

If the instruction is an *arithmetic operation*, the CPU instructs the ALU to perform that operation. To perform an `add`, for example, the ALU adds the 8-bit contents of registers A and B, and accumulates the sum in register A. An addition may generate a carry. A subtraction, on the other hand, may yield a zero or a negative. The CU sets the appropriate bits in the SR, depending on the result in register A.

If the instruction is a *conditional branch*, say one that uses relative addressing and with a loop termination condition `i < 10` where `i` is the loop index, the CU increments the loop index and subtracts 10 from the value. If the result in register A is not 0, the loop must continue. Then, the CU decrements the PC by the jump distance specified in the branch instruction, thus jumping to the top of the loop. If the register A 0, however, the CU increments the PC by 2, so the PC now holds the address of the instruction that follows the end of the loop.

***interrupt***—When the CPU receives an interrupt signal from a peripheral, the IC checks if the interrupt is an IRQ or an NMI. If the interrupt is an IRQ, the IC checks if it has sufficient priority to interrupt the currently executing code. If the interrupt is an NMI, however, the IC informs the CU to handle the interrupt, immediately.

Before handling an interrupt, the CU finishes the currently executing instruction first, and pushes the current value of the PC onto the stack, thereby suspending the execution of the code. Then, the CU loads the PC with the address of the handler procedure. The CU knows where to fetch the handler procedure's address, because the address is stored in the interrupt vector table, indexed by the interrupt number, and the interrupt vector table is located in ROM at either `0x0000` (or, `0xFFFF`, depending on the CPU design). If the interrupt number is `3`, for instance, its service handler address is located at `0x0000 + 3 * 2 bytes = 0x0006` in the vector table. Since every address is 2 bytes in length, the low order byte of the handler procedure's address is at `0x0006` and the higher order byte of the address is at `0x0007`.

The CU will execute the first instruction of the handler procedure, starting at the next fetch stage. Upon reaching the interrupt return instruction at the end of the handler procedure, the CU pops the saved PC value off the stack and loads the address into the PC. The suspended code resumes execution at the start of the next fetch stage.

# CONCLUSION

I explained in this article how a computer is built, how it boots up, how its CPU performs calculations, how these calculations when combined exhibit behaviours as complex as playing chess. My main goal here was to give software developers a better understanding of how the computer hardware executes programmes, with the hope that this low-level understanding may make them more mechanically sympathetic programmers. This hardware execution model I presented also sheds light on the inner workings of [virtualisation technologies](https://en.wikipedia.org/wiki/Hardware_virtualization), [hardware emulators](https://en.wikipedia.org/wiki/Hardware_emulation), and [virtual machines](https://en.wikipedia.org/wiki/Virtual_machine).

As much as possible, I avoided delving into the low-level details like clock signal phases, clock skew, signal degradation during transmission, serial and parallel I/O protocols, power regulation, heat dissipation, circuit layout, chip fabrication, etc. I also shunned modern, advanced topics like pipelining, branch prediction, vector processing, SIMD, VLIW, multi-core processors, on-chip instruction and data caches, in-core high-speed buses, NUMA, etc. Those interested in modern processors and microcontrollers should read textbooks like *[Computer Architecture: A Quantitative Approach](https://www.amazon.com/Computer-Architecture-Quantitative-Approach-Kaufmann/dp/0128119055/ref=sr_1_2?dchild=1&keywords=computer+architecture&qid=1614712937&sr=8-2)* and *[The Designer's Guide to the Cortex-M Processor Family: A Tutorial Approach](https://www.amazon.com/Designers-Guide-Cortex-M-Processor-Family/dp/0081006292/ref=sr_1_8?dchild=1&keywords=cortex-m&qid=1614714051&sr=8-8)*.