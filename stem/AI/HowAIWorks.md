---
title: "How Artificial Intelligence Works"
tags:
  - mathjax
use_math: true
---

# INTRODUCTION

In this article, I present a high-level overview of artificial intelligence (AI). I describe what AI is, its inner workings, its uses, and its social and ethical implications. All technical writings fall in the spectrum between scientific textbooks and newspaper reports. Textbooks are accurate, precise, and concise, but dense. Reports are vague and fluffy, but accessible. Because I wrote this article for anyone who is curious about how AI works, it is nearer to the report end of the spectrum than to the textbook end. But I tried to be as accurate as possible, without diminishing accessibility. Where appropriate, I sacrificed precision and concision for readability. More importantly, this is a "how it works" article for the general public, not a "how to work with it" guide for the practitioners. The focus, therefore, is on the simpler, classical algorithms, and not on the sophisticated, cutting-edge techniques.

# APPROACHES

There are tonnes of approaches used in AI: genetic algorithms, support vector machines, Bayesian networks, decision trees, stochastic methods, deterministic methods‚Äîthe list is long. It is also important to realise that there are many equivalent classical approaches, such as signal processing, statistical analysis, and so on, that are capable of solving the same set of problems that AI can solve.

In this article, we concentrate on the two historically significant AI traditions: connectionist and rule-based. *Connectionist* AI is commonly known as neural networks. *Rule-based* AI is traditionally associated with the LISP programming language. The most prevalent approaches currently in use are of the connectionist variety, which shall be our primary focus, here.

## *connectionist*

The myriad AI techniques that are in popular use today, different as they may appear, all share a common foundation: they all come under the umbrella term *artificial neural network* (NN). NN is a computational approach inspired by the quintessential biological neural network‚Äîthe [brain](https://en.wikipedia.org/wiki/Brain). The most basic processing unit in the brain is the [neuron](https://en.wikipedia.org/wiki/Neuron). See the figure below. The [dendrites](https://en.wikipedia.org/wiki/Dendrite) of the neuron receive electrochemical signal inputs as weighted outputs from the upstream neurons. The weight can be negative, zero, or positive. The [soma](https://en.wikipedia.org/wiki/Soma_(biology)), the cell body, sums up the weighted input signals. The neuron also receives activation signals via the dendrites. Inhibitory (negative) activation signal keeps the neuron inactive. Excitatory (positive) activation signal causes the neuron to fire its output, which is derived from the weighted sum of its inputs. The [axon](https://en.wikipedia.org/wiki/Axon) trunk carries the output signal to the [synapses](https://en.wikipedia.org/wiki/Synapse), which make electrochemical connections to the input dendrites of downstream neurons. Learning involves establishing, strengthening, weakening, and removing signal pathways among the neurons. Although the structure and functions of the brain are far more intricate and complex, the simply-butchered description above suffices for our purposes, here.

![biological neuron](../figures/AI/BiologicalNeuron.jpg)

The earliest, practical NN model was invented in 1943 by neuroscientist [McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) and logician [Pitts](https://en.wikipedia.org/wiki/Walter_Pitts). The diagram below shows the [McCulloch-Pitts](https://en.wikipedia.org/wiki/Artificial_neuron) artificial neuron. The neuron $n_j$ computes its output $o_j = f_j(i_j)$ where its transfer function is $f_j$ and its net input $i_j = ‚àë_{i=0}^{N} w_{ji} o_i$. The McCulloch-Pitts neuron uses the threshold function as its transfer function. This is the simplest model of the biological neuron's activation behaviour.

The convention in NN literature is to label input signals $i$, output signals $o$, and weights $w$. And the neurons in the adjacent upstream layer are indexed with $i$, the neurons in the current layer with $j$, and the neurons in the adjacent downstream layer with $k$. So, the labels can get a bit knotty at times‚Äî$i_i$, $o_i$, $i_j$, $o_j$, etc. Also, since a weight is associated with the connection between two neurons in adjacent layers, the input connection weights of the neuron $n_j$ are labelled $w_{ji}$ and its output connection weights are labelled $w_{jk}$. Programmers, beware!

So, by convention, the $o_i$ is an upstream neuron $n_i$'s output signal that arrives at the neuron $n_j$ via its input connection that has the multiplier weight $w_{ji}$. The neuron $n_j$ then sums all its weighted inputs $w_{ji} o_i$, and passes this net input value through its transfer function $f_j$ to compute its output $o_j$. The output $o_j$ is then sent to downstream neurons.

![McCulloch-Pitts neuron](../figures/AI/McCullochPitts.jpg)

Neurons are assembled into a neural network by arranging them in layers. Classic NNs typically used one or two layers of neurons, along with their weighted connections, as shown below. Learning involves presenting a list of input pattern vectors to the network and then modifying its connection weights in accordance with how well the network recognises each pattern. This weight adjustment method is referred as the *learning algorithm*. The final weight values produced by the algorithm encode the salient features present in the training patterns. A *feature* is an entity's distinctive attribute that can be represented compactly. It could be breadth, curvature, texture, colour, brightness, timbre, position, orientation‚Äîanything the network can use reliably to identify the entity in the context of the problem.

![neural network](../figures/AI/NeuralNetwork.jpg)

The two main types of learning algorithms are the supervised and the unsupervised varieties. A *supervised* learning algorithm requires the user to create a set of target (desired output) patterns, one for each input pattern, whereas an *unsupervised* learning algorithm does not require target patterns.

There have been many classic NN learning algorithms. The first was the unsupervised [self-organising](https://ieeexplore.ieee.org/document/1057468) learning algorithm invented by Farley and [Clark](https://en.wikipedia.org/wiki/Wesley_A._Clark) in 1954. Although unsupervised learning algorithms are less frequently used today, the [self-organising map](https://en.wikipedia.org/wiki/Self-organizing_map) (SOM) algorithm is well known. It was invented in 1982 by Finnish engineering professor [Kohonen](https://en.wikipedia.org/wiki/Teuvo_Kohonen), who was inspired by the human perceptual systems, specifically the visual and the aural. Most learning algorithms in use today, however, are of the supervised variety. They trace their origins to the [Perceptron](https://en.wikipedia.org/wiki/Perceptron), a supervised learning algorithm for linear classifiers. It was invented by American psychologist [Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) in 1957. He first implemented the algorithm on Cornell's IBM 704 mainframe. Subsequently, he built a custom, electromechanical [analogue computer](https://en.wikipedia.org/wiki/Analog_computer), called the Mark I Perceptron, which used motorised [potentiometers](https://en.wikipedia.org/wiki/Potentiometer) to represent trainable (adjustable) weights. Perceptron was capable of recognising letters. The eye of the Perceptron was a $20 √ó 20$ grid of photocells that produced a [greyscale](https://en.wikipedia.org/wiki/Grayscale) image. This resolution, though very coarse, was adequate to encode letters presented to the sensor.

Many in IT today are of the opinion that AI is a simple matter of programming to an API published by an Internet Giant. In truth, psychologists, neuroscientists, mathematicians, electrical engineers, computer scientists, and STEMers of all stripes participated in establishing and advancing the NN subfield of AI: psychologists brought forth the concept of connectionism; mathematicians provided the theoretical foundation; electrical engineers built the first neuro computers; computer scientists implemented the early simulations. Hence, AI is definitively a multi-disciplinary field of study.

## *rule-based*

The main weakness of Perceptron was its linearity: it can only learn to discern patterns if they are linearly separable, that is their classifier boundary is a straight line. The Perceptron's inability to learn even the simplest of non-linear classifiers, like the one required to implement the [exclusive or](https://en.wikipedia.org/wiki/Exclusive_or) (XOR) function shown below, led to its eventual downfall.

![non-linear classifier](../figures/AI/NonLinearClassifier.jpg)

In 1969, [Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) and [Papert](https://en.wikipedia.org/wiki/Seymour_Papert) of MIT AI Lab published their book, [*Perceptrons*](https://en.wikipedia.org/wiki/Perceptrons_(book)). This book contains mathematical proofs of why the Perceptron algorithm is incapable of solving non-linear problems. Minsky and Rosenblatt were childhood friends. Minsky dedicated his book to Rosenblatt‚Äîcheeky.

Minsky was motivated, at least in part, to show the limitations of the connectionist approach, because his AI Lab at MIT was wholly dedicated to rule-based approach. In 1958, [McCarthy](https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)) created the [LISP](https://en.wikipedia.org/wiki/Lisp_(programming_language)) programming language at MIT. Coincidentally, the first implementation of LISP was for the IBM 704. LISP was a machine implementation of [ùúÜ-calculus](https://en.wikipedia.org/wiki/Lambda_calculus), a mathematical model of computation created by [Church](https://en.wikipedia.org/wiki/Alonzo_Church) in 1936. LISP was also the second high-level programming language (after FORTRAN) and the first [functional](https://en.wikipedia.org/wiki/Functional_programming) programming language. The mathematical heritage and the [symbolic processing](https://en.wikipedia.org/wiki/Computer_algebra) abilities of LISP were well suited to implementing rule-based AI programmes, and it quickly became the favourite among the rule-based crowd, [especially at MIT](https://en.wikipedia.org/wiki/Lisp_machine). Suffice it to say, Minsky's *Perceptrons* killed off Rosenblatt's Perceptron and other NNs and, by the early 1970s, expert systems dominated AI.

An [expert system](https://en.wikipedia.org/wiki/Expert_system) is an AI application modelled upon the way human experts make technical decisions in the course of performing their daily duties. Early automated theorem proving systems, chess playing systems, and decision support systems were, in essence, expert systems. Such a system is preconfigured with a set of rules for making decisions‚Äîselecting a `yes` or a `no` at each step. When the user provides an input, the system applies the rules against the input and, step by step, arrives at a decision. When the set of rules is comprehended and comprehensive, expert systems work exceedingly well. Another major advantage of expert systems is that the results can always be explained fully by simply tracing the algorithm's path down the decision tree.

But few things in nature exhibit such orderly behaviour. Moreover, humans understand even fewer things in nature at such a level of detail. Even if we manage to grasp the dynamics of a large problem, the rules and their interactions would be intractable. Unfortunately, the purveyors of expert systems over promised and the media over reported the hype. By the early 1980s, the users and the public had lost interest in expert systems, and the funding spigot was shut. This period is now known as [AI Winter](https://en.wikipedia.org/wiki/AI_winter).

## *backpropagation*

A few researchers who saw the true strengths of AI continued working in the field‚Äîunderground. Then, in 1986, [Rumelhart](https://en.wikipedia.org/wiki/David_Rumelhart), [Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), and [Williams](https://en.wikipedia.org/wiki/Ronald_J._Williams) published their seminal paper, *[Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)*. Their learning algorithm became known as *backpropagation* (BP). BP is a supervised learning algorithm that modifies the network's weights in proportion to the amount of representation error the network commits in each training pass through the data set. In that respect, BP's learning process is no different in kind than that of the Perceptron: find the optimal set of weights that minimise representation errors. But BP's main advantage over the Perceptron is its ability to learn non-linear classifiers. Indeed, non-linearity was why BP works and Perceptron did not. Mathematically, the BP learning algorithm is a simple, [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) optimisation scheme known to mathematicians since [Cauchy](https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy) introduced it in 1847. But practically, it was capable of solving substantive problems which, until then, were deemed too difficult for machines.

In the 1980s, a room-sized minicomputer, like the [DEC VAX-11/780](https://en.wikipedia.org/wiki/VAX-11), had 1 MB of RAM, usually much less. So, networks were very small, a typical model having just two layers and a handful of neurons in each layer. Despite such restrictions, BP-based networks were able to solve efficiently a number of complex, non-linear problems, like speech recognition, image classification, robotics control, etc. Within a couple of years, BP single-handedly resuscitated the dying AI field. Many variants and improvements soon followed, further improving network performance. Understandably, the AI community was cautiously optimistic, this time round.

Most users of NNs simulate on mainstream computing hardware, like the VAX-11 or RISC workstations. But these machines, though powerful for their day, are severely limited in both CPU power and RAM capacity. In response, a number of groups developed custom hardware for neural computing. Some built machines out of common digital hardware but optimised for neural computing, but others designed custom analogue VLSI chips. The Stanford group led by [Mead](https://en.wikipedia.org/wiki/Carver_Mead) was especially productive in this area. His PhD student, [Mahowald](https://en.wikipedia.org/wiki/Misha_Mahowald), developed the [silicon retina](https://en.wikipedia.org/wiki/Event_camera), which is an image sensor technology that out performs the human eye and the best DSLRs by several orders of magnitude. This technology was further developed by Mead's company, [Foveon](https://en.wikipedia.org/wiki/Foveon), which is now part of [SIGMA](https://www.sigma-global.com/en/).

Most folks today are unaware that there was a AI Mini Ice Age during the mid 1990s. Although the hardware capabilities had increased leaps and bounds, the storage technology was still somewhat limited. Disc sizes were still measured in MB, albeit in hundreds thereof. Most data sets used in AI were only a few MBs in size. The lack of storage capacity necessarily constrained the completeness of data sets. So, neither humans nor networks could see the true nature of massive, complex, dynamical systems. The lack of data, thus, stunted the growth of AI. To get around some of the limitations, many experimented melding rule-based and connectionist techniques. But for the most part, NNs were confined to academic research and to small-scale commercial use to the end of the 1990s. The [dot-com collapse](https://en.wikipedia.org/wiki/Dot-com_bubble) at the end of the 1990s and the subsequent economic stagnation did no favours to AI.

## ***deep learning***

Then came the age of Internet Giants‚Äîthe early 2000s. Massive, simultaneous advances in computing power, storage capacity, network connectivity, graphics technology, and software techniques conspired to create the fastest growth in the history of human invention. The Web came of age. Online shopping became the norm. Billion-user social media platforms became a reality. The Internet Giants saw gold in the users' private data, which they procured by giving away free online services. [Big data](https://en.wikipedia.org/wiki/Big_data) was born. By 2010s, everything converged: massive amounts of collected data, advances in NN algorithms, huge rise in processing power. It was in this Era of Data that people began recognising the true potential of BP, or rather its descendant‚Äî[deep learning](https://en.wikipedia.org/wiki/Deep_learning) (DL).

Whereas the classic BPs of the 1990s could only use two or three layers of neurons due to hardware limitations, the DLs of the 2010s routinely employ many layers and many different types of neurons. Today, home users of DLs train their networks on powerful, but inexpensive, graphics cards, like the [nVIDIA RTX](https://www.nvidia.com/en-us/design-visualization/desktop-graphics/) so as to exploit the GPU's built-in support for parallel matrix operations across tens of thousands of cores. The "deep" in deep learning refers to the many layers of neurons, as shown below in grey boxes.

![deep learning neural network](../figures/AI/DeepLearningNetwork.jpg)

Another substantial advantage of modern DLs over classic BPs is feature extraction. Feature extraction required the use of sophisticated, mathematical techniques such as [digital signal processing](https://en.wikipedia.org/wiki/Digital_signal_processing) (DSP), [digital image processing](https://en.wikipedia.org/wiki/Digital_image_processing) (DIP), [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP), etc. Each such technique constitutes a highly specialised, not to mention expansive, subfield within mathematics or engineering. So, competent practitioners were relatively few in numbers in the early days of BPs, which limited their usefulness. But DLs subsume feature extraction into the front-end layers of the network. That is, a DL network automatically extracts salient features from the data, and learns to represent those features in its connection weights, with little or no guidance from human experts. Hence, DL networks can be used effectively even by non-experts. Paradoxically, this characteristic of DLs is also a disadvantage, of sort: if a complex AI problem can be solved merely by clicking a button on a GUI, there is no need for the user to perform any analysis of the problem domain and of the massive amounts of data accumulated, so he gains no insight into the problem and he cannot explain the reasons why the solution appears to work. Regardless, DLs are here to stay; now is their day.

Over the past quarter century, our shopping habits, social media propensities, blogging activities, email exchanges, cloud-borne personal information‚Äîjust about every aspect of our lives, both personal and professional‚Äîhave been collected, analysed, categorised, tracked, and predicted by Internet Giants' DLs. Despite their prominence in modern life, however, DLs are but a small part of a much broader category of powerful modern methods known collectively as [machine learning](https://en.wikipedia.org/wiki/Machine_learning) (ML) algorithms, which in turn are a subfield within AI. Suffice it to say, AI is an expansive field of study.

# APPLICATIONS

There are three broad categories of AI applications: approximating, classifying, and forecasting. Each category has multiple subcategories, which we shall examine, below.

Just as there are applications, there are appliers, the users of AI: the practitioners. Researchers invent AI algorithms. AI software designers use those algorithms to build industry-scale systems. Coders use published libraries to implement pieces of code that are composed into large systems. Companies employ researchers, designers, and coders to exploit AI. In terms of skills, it does not take much to become an AI practitioner: calculus, linear algebra, probability, and statistics. But an AI practitioner earns his keep by analysing and understanding complex, dynamic behaviours of large systems from the massive amounts of collected raw data. That is difficult, even for the skilled and the experienced.

The Internet Giants who exploit AI technologies often state that the real beneficiaries of AI are the members of the public. A closer look, however, reveals that the public is the victim.

## *approximating*

*Approximating* means the AI algorithm learns to emulate the behaviour of a real-world system well enough that its behaviour is indistinguishable from the real thing in most cases. There is a set of important, optimisation problems that have been proven to be exceedingly difficult for computers to solve. They are known as $NP$ ([non-deterministic polynomial time](https://en.wikipedia.org/wiki/NP_(complexity))) problems. That is a fancy way of saying computer scientists do not know how to create efficient, exact solutions to these problems because they are much too complicated, and they must resort to approximate solutions. The [vehicle routing problem](https://en.wikipedia.org/wiki/Vehicle_routing_problem) is a famous real-world scenario that is computationally difficult. The goal is to find an optimal set of routes for a fleet of delivery vans to deliver parcels to a set of customers. The larger the network, the more difficult the problem. This is what transportation service companies face, daily.

In such situations, NNs can efficiently learn to approximate the complex behaviour of difficult problems. Both conventional algorithms and NNs yield approximate solutions that are acceptable, but not necessarily optimal. But NNs are computationally more efficient.

## ***classifying***

*Classifying* means the AI algorithm organises its input patterns into predefined classes. An instance of classification problem is sorting bananas into A, B, or C grades, in accordance with their cultivar, grower, shape, size, weight, colour, and ripeness. A conventional system might cast this as a multi-dimensional constraint satisfaction problem, a rather hard problem to solve. An NN-based system can solve this problem with ease and speed.

A variant of classifying is *clustering*, where we do not know the input classes, a priori. Clustering can be used as a preprocessing stage: a representative subset of the unknown data set can clustered to discover the number of classes present in the data, for example. Traditional, $k$-means clustering technique can solve this problem, but it requires the user to select the total number of classes as a limit and it falters when the number of input samples are extremely large. An unsupervised NN algorithm, like Kohonen's SOM, can solve this problem much more efficiently, and without any user intervention. The output of SOM is a 2D projection of the $n$-dimensional input data set.

Another variant of classifying is *identifying*, for example, to identify the faces in a photograph or to identify a person by his voice. Rosenblatt's Perceptron was perhaps the first practical system capable of identifying individual English letters from images.

Still another variant of classifying is *recognising*: text recognition, speech recognition, and so on. Recognition is much more difficult than identification. Whereas a computer can identify the letter "a" in an image or a phoneme "a" in an audio clip relatively easily, but recognising the word that comprises several letters or an utterance containing multiple phonemes is orders of magnitude harder for the computer. This difficulty is due to context dependency. To recognise a word, the computer must not only identify the constituent letters, it must also recognise their relationships within the word. In the 1980s, several researchers, Kohonen, Grossberg, LeCun, and others, invented NNs capable of simple speech recognition. Those systems required lots of manual preprocessing of audio. Today, however, DL networks can perform sophisticated speech recognition, with little or no preprocessing.

Note that NNs, despite their powers, are incapable of explaining how and why they produced the results. There are applications where that information is necessary. For example, if the bank denies a loan, the bank must justify its actions and explain to the applicant that the actions comply with the law. The intractability of large NNs precludes detailed analyses.

## *forecasting*

*Forecasting* means the AI algorithm, having learned the relevant trends from historical data, can predict a future outcome, given a particular input. That a large, national retail store must stock the shelves in all its stores for the holiday shopping season is obvious; that requires no AI. But in today's global economy, the supply chain is affected by many unforeseen events occurring all across the world. This is where forecasting is necessary. Indeed, forecasting is used today by all large entities, public and private.

The massive amounts of data now available makes forecasting possible, even if it is still not an easy problem. If forecasting were easy, we would not suffer from hurricanes, tsunamis, earthquakes, volcano eruptions, and other "unforeseen" natural disasters. Such problems are difficult, even for NNs, because they are chaotic. A chaotic system, like a storm, is not random. A storm's present behaviour can be analysed using deterministic physical laws, without resort to randomness. But its behaviour is so sensitive to the initial conditions that it is impossible to predict the exact path of the storm. This is why weather forecasting is as much an art as it is a science. NNs are no panacea to solving chaotic problems.

But NNs can indeed be used to make predictions in deterministic systems, such as in the *control* of robots and other autonomous vehicles. Control systems are used to govern the dynamic behaviours of mechanical systems: from a two-limb, laboratory robot arm to a large, automated factory floor; from a self-driving car to a spacecraft reentry system. In large, complex applications, the control system cannot simply react to the current conditions; it must be capable of learning from past events, predicting future events, and reacting to novel events that it has never seen before. It is extremely difficult to built into the system this level of flexibility, using conventional techniques. NN-based systems, though, are capable of orders of magnitude greater flexibility.

A more recent use of NNs is in *composition*. It is now possible to train an NN on the text of a dead, famous author, and have the network generate new writings in the style of that author. Similarly, new musical compositions and new paintings can be generated, automatically. [Deepfakes](https://youtu.be/T76bK2t2r8g) that permeate social media today are a compositional, as well. Still in their nascent stage, they have already demonstrated their ability to fool the gullible.

# ALGORITHMS

categories‚Äîstatistics, connectionist, hybrid, etc.

classic techniques‚ÄîSOM, BP, analogue VLSI

modern techniques‚ÄîDL

data collection, analysis, cleansing‚Äîstatistics, projection, DSP, DIP, wavelets

# ALERTNESS

Life is full of dangers, and every new and useful invention adds a set of unique dangers thereto. Knives and scissors and hammers are dangerous. So are [trains and boats and planes](https://www.youtube.com/watch?v=BlbS0JNFMIM). But they are all essential to modern life. AI is no different; it is a sharp tool that, if mishandled, will gouge the user's hand.

AI as a research discipline has been around for nearly a century. But as a practical tool, AI is only about a decade old. Despite its youth, AI already controls many important aspects of our lives. But the fact that the most successful uses of AI have been invading privacy, deep faking, and e-commerce profiteering‚Äînot tackling homelessness, global pandemics, or climate change‚Äîspeaks volumes about us humans.

Researchers (inventors) and users (exploiters) alike must be cognisant of the fact that AI is nothing miraculous, but that it is a human invention susceptible to myriad human frailties: overenthusiasm, overconfidence, oversight, and all manner of overages. Being just a tool, AI can be used and misused with equal dexterity. But whereas the misuse of a pair of scissors harms only the user, the [misuse of AI has the power to dismantle democracy](https://www.independent.co.uk/news/long_reads/artificial-intelligence-democracy-elections-trump-brexit-clinton-a7883911.html).

The secret algorithms currently being used by the social media platforms are AI, specifically the DL variety. These algorithms are very good at showing the user what he wants to see. But these algorithms are not trained to show the user what he ought to see, because there is no money in that. Social media platforms profit by exploiting the billions of users' wants, but they effectively distort the truth. Access to reliable facts is the fundamental building block of a free, democratic society. It is not that the truth is a rare commodity today, but that the truth is now very difficult to discern from the much more abundant falsehoods. This erodes the public's trust in authorities, and this lack of faith in the institutions results in a feedback: the uncontrolled spread of feel-good falsehoods on social media.

Then, there are [deepfakes](https://en.wikipedia.org/wiki/Deepfake). They are an interesting challenge, perhaps even an artistic expression, for the creators. But in nefarious hands, this technology can precipitate ethnic cleansing and other [crimes against humanity](https://en.wikipedia.org/wiki/Crimes_against_humanity). Granted, there exist a few sophisticated forensic techniques that can detect deepfakes, but these tools are not perfect and they are available only to a handful of specialists. Hence, bad actors have plenty of time and opportunities to achieve their ill goals by exploiting the rampant irrationality of the unsophisticated, but well connected, masses.

Today, we conduct our lives online: finances, commerce, and even voting. Everything we do online is traced and trended by entities large and small, and those pieces of data are traded openly and surreptitiously. It is now remarkably simple to use the already-available data and AI software to devise a [gerrymander](https://en.wikipedia.org/wiki/Gerrymandering) our electoral boundaries, thereby favouring a fringe political group or perhaps even a foreign enemy. Currently, there are governments who [employ AI to oppress minorities and dissidents](https://www.pbs.org/wgbh/frontline/article/how-chinas-government-is-using-ai-on-its-uighur-muslim-population/). Researchers from Western, democratic countries supply the technologies to those oppressors.

In the Age of Data, AI has "learned to seize" an immense amount of power for itself, but it possess not a tincture of morals. And the entities exploiting AI, likewise, have powers but no morals. As such, [We the People](https://en.wikipedia.org/wiki/Preamble_to_the_United_States_Constitution) must exercise our duty to study, monitor, and regulate the conduct of those who wield the powers of AI. As a still-free folk, we have the moral duty not only to preserve our own freedoms but also to protect the human rights of others. Woe be upon those who equate "freedom" with "free".

# CONCLUSION

There are three types of people interested in AI: the curious public, the incurious users, and the eager researchers. The curious public are those who do not work with AI but just want to understand how AI works at a basic level. The incurious users are coders in the IT industry who could care less about how AI works and they just want to slap together some AI programme. The eager researchers are the ones who have their skins in the game. Although this article is aimed at the curious public, the other two groups may benefit from it, as well: coders and their managers may benefit from understanding better what the code is doing; aspiring researchers and beginning graduate students may benefit from knowing the historical background of their field of interest.

Some years ago, I was shooed off the NN forum on the now-defunct Google+ platform by a gang of young AI enthusiasts for suggesting that all researchers should be familiar with the classic works of researchers, like McCulloch, Pitts, Rosenblatt, Widrow, Kohonen, Hopfield, Rumenhart, LeCun, etc., in addition to the modern literature. Despite my resounding defeat on social media, I stand by that view: they who admire only the newest and the most fashionable learn nothing from history, and they lack the requisite background of the field. It is important to realise that knowing how to code an AI application using library calls is not the same as understanding what that application is actually doing. Those who use AI without comprehending its strengths, weaknesses, and ethical implications bring no long-term benefits to society. Indeed, they may do more harm than good to humanity.

Lastly, the public must recognise that the founders and leaders of Internet Giants do not necessarily understand the AI technologies their companies are employing. The chief executive of a corporation is legally responsible for maximising the shareholders' returns. This legal mandate often conflicts with ethical, social, and equitable concerns of the public. The IT executives should wield their powers, by all means; but they ought to act in a socially responsible manner. When these large, powerful international conglomerates wilfully breach the public's trust, the only recourse is the law. In a democracy, the law derives its powers from the public. As such, the public must exercise its duty to study the impacts and trends of AI technologies and to monitor the activities of AI users.
